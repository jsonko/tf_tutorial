{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Progress는 학습 중, 이후에 모두 저장될 수 있다. 이는 곧 모델은 긴 학습 시간 중간에 멈춘 곳에서 다시 시작될 수 있다는 것을 나타낸다. \n",
    "\n",
    "모델을 저장을 통해서 모델을 공유할 수 있고, 다른 작업에 재활용할 수도 있다. 연구 모델이나 기술들을 공개할 때, 대부분의 머신러닝 엔지니어들은 다음을 공유한다.\n",
    "\n",
    "- 모델을 생성 하는 코드 \n",
    "- 학습된 weights 나 parameters \n",
    "\n",
    "Tensorflow model을 저장하는 방법은 사용한 API에 따라서 달라질 수 있다. 여기서는 tf.keras를 사용한 예제를 다룰 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <U>Setup</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/usr/local/lib/python3.6/dist-packages/yaml'\r\n",
      "Consider using the `--user` option or check the permissions.\r\n",
      "\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q h5py pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                 loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <U>Save checkpoints during training</U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주요한 Use case 는 학습 도중과 끝에 자동적으로 checkpoints를 저장하는 것이다. 이 방법을 사용하면 retrain 없이 학습된 모델을 사용할 수 있고, 학습 과정이 갑자기 중단된 경우에도 이어서 학습을 진행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint callback usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.1855 - acc: 0.6530 - val_loss: 0.7320 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00001: saving model to training_1/cp.cpkt\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 193us/step - loss: 0.4333 - acc: 0.8790 - val_loss: 0.5617 - val_acc: 0.8290\n",
      "\n",
      "Epoch 00002: saving model to training_1/cp.cpkt\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 205us/step - loss: 0.2888 - acc: 0.9310 - val_loss: 0.4906 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00003: saving model to training_1/cp.cpkt\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 203us/step - loss: 0.2045 - acc: 0.9440 - val_loss: 0.4421 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00004: saving model to training_1/cp.cpkt\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 204us/step - loss: 0.1546 - acc: 0.9690 - val_loss: 0.4414 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00005: saving model to training_1/cp.cpkt\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 212us/step - loss: 0.1161 - acc: 0.9830 - val_loss: 0.4209 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00006: saving model to training_1/cp.cpkt\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 228us/step - loss: 0.0870 - acc: 0.9910 - val_loss: 0.4166 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00007: saving model to training_1/cp.cpkt\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 195us/step - loss: 0.0661 - acc: 0.9930 - val_loss: 0.4221 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00008: saving model to training_1/cp.cpkt\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 184us/step - loss: 0.0493 - acc: 0.9950 - val_loss: 0.4098 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00009: saving model to training_1/cp.cpkt\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 220us/step - loss: 0.0380 - acc: 0.9980 - val_loss: 0.4114 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00010: saving model to training_1/cp.cpkt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efde5c3f518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_1/cp.cpkt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10,\n",
    "         validation_data = (test_images, test_labels),\n",
    "         callbacks= [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint  cp.cpkt.data-00000-of-00001  cp.cpkt.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 Untrained model을 생성하자. Only weights 로부터 모델을 복원할 때, 반드시 original model과 동일한 모델을 사용해야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 115us/step\n",
      "untrained model, accuracy : 1.4e+01%\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"untrained model, accuracy : {:5.2}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 학습한 checkpoint로 부터 weight를 load해서 다시 평가해보면,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 42us/step\n",
      "Restored model, accuracy: 86.70%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "loss,acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint callback options\n",
    "\n",
    "callback은 checkpoints 에 unique name을 지정하고, 체크포인트를 저장하는 빈도를 조절하는 options을 제공한다.\n",
    "\n",
    "새로운 모델을 학습하고, 매 5 epoch 마다 다른 이름을 지정하여 checkpoint를 저장해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: saving model to training_2/cp-0005.ckpt\n",
      "\n",
      "Epoch 00010: saving model to training_2/cp-0010.ckpt\n",
      "\n",
      "Epoch 00015: saving model to training_2/cp-0015.ckpt\n",
      "\n",
      "Epoch 00020: saving model to training_2/cp-0020.ckpt\n",
      "\n",
      "Epoch 00025: saving model to training_2/cp-0025.ckpt\n",
      "\n",
      "Epoch 00030: saving model to training_2/cp-0030.ckpt\n",
      "\n",
      "Epoch 00035: saving model to training_2/cp-0035.ckpt\n",
      "\n",
      "Epoch 00040: saving model to training_2/cp-0040.ckpt\n",
      "\n",
      "Epoch 00045: saving model to training_2/cp-0045.ckpt\n",
      "\n",
      "Epoch 00050: saving model to training_2/cp-0050.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efdd00d2898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True,\n",
    "                                                period=5) # Save weights every 5 epoch\n",
    "\n",
    "model = create_model()\n",
    "model.fit (train_images, train_labels,\n",
    "          epochs = 50, callbacks = [cp_callback],\n",
    "          validation_data = (test_images, test_labels),\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 이제 저장된 checkpoints를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('training_2/cp-0030.ckpt'),\n",
       " PosixPath('training_2/cp-0035.ckpt'),\n",
       " PosixPath('training_2/cp-0040.ckpt'),\n",
       " PosixPath('training_2/cp-0045.ckpt'),\n",
       " PosixPath('training_2/cp-0050.ckpt')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "checkpoints = pathlib.Path(checkpoint_dir).glob(\"*.index\")\n",
    "checkpoints = sorted(checkpoints, key=lambda cp: cp.stat().st_mtime)\n",
    "checkpoints = [cp.with_suffix('') for cp in checkpoints]\n",
    "latest = str(checkpoints[-1])\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 87us/step\n",
      "Restored model, accuracy: 87.80%\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights(latest)\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <U>What are these files?</U>\n",
    "\n",
    "위 code는 오직 학습된 weights 를 binary 포멧으로 저장하는 checkpoint-formatted file 로 weight를 저장한다. \n",
    "index 파일은 weights 가 어느 shard에 저장된 것인지 나타내 준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <U>Manually save weights</U>\n",
    "\n",
    "수동으로 weight를 저장하는 방법은 `Model.save_weights` 를 호출하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 91us/step\n",
      "Restored model, accuracy: 87.80%\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <U>Save the entired model</U>\n",
    "\n",
    "전체 모델은 weight values, model configuration, optimizer configuration 을 포함하는 하나의 파일로 저장할 수 있다. 이 방법을 사용하면 checkpoint 파일로 저장하고, 나중에 학습을 다시 이어서 진행할 수 있다. \n",
    "\n",
    "Keras에서 fully-functional model을 저장하는 것은 매우 유용하다. (이 파일을 Tensorflow.js 에서 가져와서 browser에서 학습하고 실행시킬 수 있다.)\n",
    "\n",
    "Keras는 HDF5 표준을 사용한 기본 저장 포멧을 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 458us/step - loss: 1.1971 - acc: 0.6600\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 169us/step - loss: 0.4202 - acc: 0.8810\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 167us/step - loss: 0.2771 - acc: 0.9290\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 0.2044 - acc: 0.9560\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 180us/step - loss: 0.1520 - acc: 0.9680\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장한 H5 파일에서 모델을 recreate 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = keras.models.load_model('my_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 113us/step\n",
      "Restored model, accuracy: 85.40%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = new_model.evaluate(test_images, test_labels)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방법을 통해서 모델의 모든것을 저장할 수 있다. \n",
    "- weight values\n",
    "- Model's configuration\n",
    "- Optimizer configuration\n",
    "\n",
    "Keras는 architecture inspecting을 통해서 모델을 저장할 수 있다. 그리고 현재 TensorFlow optimizer(tf.train) 은 저장할 수는 없다. 이를 사용할 때는 loading한 이후에 re-compile해야 한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
